---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## ğŸ˜ About Me (æäº‘é‘«)

I am a Tenure-Track Associate Professor at Harbin Institute of Technology, Shenzhen. I earned my Ph.D. from the Harbin Institute of Technology, Shenzhen, advised by Prof. [Baotian Hu](https://homepage.hit.edu.cn/hubaotian), Prof. [Yuxin Ding](https://homepage.hit.edu.cn/dingyuxin), and Prof. [Min Zhang](https://homepage.hit.edu.cn/MinZhang). I obtained a Master of Engineering degree from Harbin Institute of Technology, Shenzhen and a Bachelor of Science degree from Harbin Institute of Technology. Long-term cooperation with Dr. [Lin Ma](https://forestlinma.com/), Meituan, Beijing; Prof. [Wenhan Luo](https://whluo.github.io/), HKUST; Dr. [Longyue Wang](https://www.longyuewang.com/), Alibaba Group; Dr. [Yuxiang Wu](https://www.yuxiangwu.com/), 

The long-term goal of my research is to help humans with more capable artificial intelligence. Dream of building an intelligent metaverse and interesting research directions, including:
- Multimodal Reasoning and Planning
- Omnimodal Large Model
- Multimodal Agent
- Embodied Intelligence

<p style="color: blue; margin: 0;">I am actively seeking cooperators (æœ¬ç§‘ç”Ÿã€ç¡•å£«ç”Ÿã€åšå£«ç”Ÿ) who share my interest in developing large multimodal reasoning models to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments. ğŸ“§Email: liyx@hit.edu.cn </p>

<p style="color: red; margin: 0;">
å“ˆå·¥æ·±è®¡ç®—ä¸æ™ºèƒ½ç ”ç©¶é™¢<b>Lycheeå¤§æ¨¡å‹å›¢é˜Ÿ</b>é•¿æœŸæ‹›æ”¶å¤§æ¨¡å‹ç¡•åšç ”ç©¶ç”Ÿã€æœ¬ç¡•åšå®ä¹ ç”Ÿã€‚å›¢é˜Ÿå¯ä¾é å“ˆå·¥æ·±ã€æ·±åœ³æ²³å¥—äººå·¥æ™ºèƒ½å­¦é™¢ã€é¹ç¨‹å®éªŒå®¤ã€ä¸­ç§‘é™¢ä¿¡å·¥æ‰€ã€è‹å·å¤§å­¦ç­‰é«˜æ ¡å’Œå®éªŒå®¤è”åˆåŸ¹å…»ï¼Œåšæœ‰ä»·å€¼çš„ç§‘ç ”ï¼Œæ¬¢è¿ç§¯æè¸Šè·ƒæŠ¥åï¼
</p>


## ğŸ”¥ News
- 2025.11: âœ¨ The omnimodal large model [Uni-MoE-2.0-Omni models](https://arxiv.org/abs/2511.12609) (five checkpoints) are open-sourced
- 2025.10: âœ¨ Our unified speech and music generative model [Uni-MoE-Audio](https://arxiv.org/abs/2510.13344) is open-sourced 
- 2025.10: âœ¨ One long paper about the Temporal RAG Benchmark is accepted by [Nature Scientific Data](https://www.nature.com/articles/s41597-025-06098-y)
- 2025.08: âœ¨ One long paper about Temporal RAG is accepted by [CIKM 2025](https://dl.acm.org/doi/10.1145/3746252.3761292)
- 2025.08: âœ¨ The long video generation work Animaker is accepted by [ACM SIGGRAPH Asia 2025](https://animaker-dev.github.io/)
- 2025.05: âœ¨ Unified multimodal LLMs Uni-MoE is accepted by [IEEE TPAMI 2025](https://ieeexplore.ieee.org/document/10887014)
- 2025.05: âœ¨ VideoVista-CulturalLingo is accepted by [ACL 2025 Main Conference](https://videovista-culturallingo.github.io/)
- 2025.01: âœ¨ Pioneering GUI model [UI-TARS](https://github.com/bytedance/UI-TARS) is open-sourced
- 2024.11: âœ¨ Anim-Director is accepted by [ACM SIGGRAPH Asia 2024](https://dl.acm.org/doi/abs/10.1145/3680528.3687688)
- 2024.05: âœ¨ Cognitive Visual-Knowledge Alignment work is accepted by [ACL 2024 Main Conference](https://aclanthology.org/2024.acl-long.411/)
- 2024.04: âœ¨ VisionGraph is accepted by [ICML 2024 Main Conference](https://proceedings.mlr.press/v235/li24ab.html)
- 2024.04: âœ¨ Multimodal LLMs LMEye is accepted by [IEEE TMM 2024](https://ieeexplore.ieee.org/document/10598361)
- 2024.02: âœ¨ Multimodal E-commerce model is accepted by [LREC-COLING 2024](https://aclanthology.org/2024.lrec-main.77/)
- 2023.08: âœ¨ Multimodal Event Extraction work is accepted by [ACM MM 2023](https://arxiv.org/abs/2306.08966)
- 2023.05: âœ¨ Two multimodal reasoning papers are accepted by [ACL 2023 Main Conference](https://aclanthology.org/people/yunxin-li/)
- 2022.08: âœ¨ Chunk-aware reasoning work is accepted by [ACM MM 2022](https://arxiv.org/abs/2207.11401)
- 2022.05: âœ¨ Pivotal information recalling-based medical dialogue generation is accepted by [SIGKDD](https://dl.acm.org/doi/10.1145/3534678.3542674)
- 2022.03: âœ¨ Deep Spatial & Contextual Information network is accepted by [IEEE TMM 2023](https://ieeexplore.ieee.org/document/9682541)


## ğŸ“• Selected Publications 

- **[Technical Report, ArXiv 2025]** Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data<br>
<u><strong>Yunxin Li</strong></u>, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, Baotian Hu, Min Zhang<br>
**[[pdf](https://arxiv.org/abs/2511.12609)]** **[[web](https://idealistxy.github.io/Uni-MoE-v2.github.io/)]** **[[code](https://github.com/HITsz-TMG/Uni-MoE)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Uni-MoE.svg?style=social">

- **[Survey, ArXiv 2025]** Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models<br>
  <u><strong>Yunxin Li</strong></u>, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2505.04921)]** **[[web](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)]** **[[huggingface](https://huggingface.co/papers/2505.04921)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models.svg?style=social">

- **[IEEE TPAMI 2025]** Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts<br>
  <u><strong>Yunxin Li</strong></u>, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2405.11273)]** **[[web](https://uni-moe.github.io/)]** **[[code](https://github.com/HITsz-TMG/Uni-MoE)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Uni-MoE.svg?style=social">

- **[ACL 2025]** VideoVista: A Versatile Benchmark for Video Understanding and Reasoning<br>
  <u><strong>Yunxin Li</strong></u>, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2406.11303)]** **[[web](https://videovista.github.io/)]** **[[code](https://github.com/HITsz-TMG/VideoVista)]**
  
- **[SIGGRAPH Asia 2024]** Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation<br>
  <u><strong>Yunxin Li</strong></u>, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2408.09787)]** **[[code](https://github.com/HITsz-TMG/Anim-Director)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Anim-Director.svg?style=social">

- **[ACL 2024]** Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment<br>
  <u><strong>Yunxin Li</strong></u>, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2402.13561)]** **[[code](https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper)]**

- **[ICML 2024]** VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2405.04950)]** **[[code](https://github.com/HITsz-TMG/VisionGraph)]**

- **[IEEE TMM 2024]** LMEye: An Interactive Perception Network for Large Language Models<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.03701)]** **[[code](https://github.com/YunxinLi/LingCloud)]**

- **[LREC-COLING 2024]** A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2402.13587)]** **[[code](https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning)]**

- **[ArXiv 2023]** Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Wei Wang, Xiaochun Cao, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2311.15759)]** **[[code](https://github.com/HITsz-TMG/MKS2-Multimodal-Knowledge-Storage-and-Sharing)]**


- **[ACM MM 2023]** Training Multimedia Event Extraction With Generated Images and Captions<br>
  Zilin Du, <u><strong>Yunxin Li</strong></u>, Xu Guo, Yidan Sun, Boyang Li<br>
  **[[pdf](https://arxiv.org/pdf/2306.08966.pdf)]** **[[code](https://github.com/ZILIN003/CAMEL)]**

- **[ACL 2023]** A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Yuxin Ding, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.02265)]** **[[code](https://github.com/YunxinLi/NDCR)]**

- **[ACL 2023]** A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Xinyu Chen, Yuxin Ding, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.04530)]** **[[code](https://github.com/YunxinLi/Multimodal-Context-Reasoning)]**

- **[ACM MM 2022]** Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations<br>
  Qian Yang#, <u><strong>Yunxin Li#</strong></u>, Baotian Hu, Lin Ma, Yuxing Ding, Min Zhang<br>
  **[[pdf](https://dl.acm.org/doi/abs/10.1145/3503161.3548284)]** **[[code](https://github.com/HITsz-TMG/ExplainableVisualEntailment)]**

- **[SIGKDD 2022]** Medical Dialogue Response Generation with Pivotal Information Recalling<br>
  Yu Zhao#, <u><strong>Yunxin Li#</strong></u>, Yuxiang Wu, Baotian Hu, Qingcai Chen, Xiaolong Wang, Yuxin Ding, Min Zhang<br>
  **[[pdf](https://dl.acm.org/doi/abs/10.1145/3534678.3542674)]** **[[code](https://github.com/HITsz-TMG/MedPIR)]**

- **[IEEE TMM 2022]** Fast and Robust Online Handwritten Chinese Character Recognition with Deep Spatial & Contextual Information Fusion Network<br>
  <u><strong>Yunxin Li</strong></u>, Qian Yang, Qingcai Chen, Baotian Hu, Xiaolong Wang, Yuxin Ding, Lin Ma<br>
  **[[pdf](https://ieeexplore.ieee.org/abstract/document/9682541)]**
  

## ğŸ’¼ Research Internship

- HKUST Research Assistant (2025.03 - 2025.08)
- ByteDance Doubao (Seed) Team (2024.10 - 2025.02)
- Tencent AILab (2024.04 - 2024.08)
- Tencent PCG (2021.10 - 2022.06)

## ğŸ’ Service

- Conference Reviewer: ACL ARR (2023-), ICLR (2023-), NeurIPS (2024-), ICML (2024-), AAAI (2024-), ACM SIGGRAPH (2025-), CVPR (2025-), ACM MM (2023-), and IJCAI (2023-).
- Journal Reviewer:  ACM Computing Survey, IEEE TPAMI, IEEE TIP, IEEE TMM, IEEE TNNLS, IEEE TCSVT, IEEE TAI, and Neural Networks.

## ğŸ… Award

- Provincial Outstanding Graduates, 2019
- National Scholarship (2018, 2021, 2024)
- Baidu Scholarship (Global Top 40), 2024
- Huawei TopMinds (åä¸ºå¤©æ‰å°‘å¹´), 2025
- JD TGT (äº¬ä¸œé¡¶å°–æŠ€æœ¯äººæ‰è®¡åˆ’), 2025
- Tencent Qingyun (è…¾è®¯é’äº‘äººæ‰è®¡åˆ’), 2025
- Young Talent Support Project-Doctor (é¦–å±Šä¸­å›½ç§‘åé’æ‰˜åšå£«ç”Ÿè®¡åˆ’), CAST, 2024
- Outstanding Doctoral Dissertation of HIT (å“ˆå·¥å¤§ä¼˜åš), 2025




