---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## üòé About Me (Êùé‰∫ëÈë´)

I just got my Ph.D. from Harbin Institute of Technology, Shenzhen and advised by Prof. [Baotian Hu](https://homepage.hit.edu.cn/hubaotian), Prof. [Yuxin Ding](https://homepage.hit.edu.cn/dingyuxin), and Prof. [Min Zhang](https://homepage.hit.edu.cn/MinZhang). I obtained a Master of Engineering degree from Harbin Institute of Technology, Shenzhen and a Bachelor of Science degree from Harbin Institute of Technology. Long-term cooperation with Dr. [Lin Ma](https://forestlinma.com/), Meituan, Beijing; Prof. [Wenhan Luo](https://whluo.github.io/), HKUST; Dr. [Longyue Wang](https://www.longyuewang.com/), Alibaba Group; Dr. [Yuxiang Wu](https://jimmycode.github.io/), University College London.

The long-term goal of my research is to help humans with more capable artificial intelligence. Dream of building an intelligent metaverse and interesting research directions including:
- Multimodal Collaborative Reasoning
- Video Understanding and Generation
- Multimodal Agent
- Embodied Intelligence

<ul style="color: blue;">I am actively seeking cooperators who share my interest in developing large multimodal reasoning models to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.</ul>


## üî• News
- 2025.10: ‚ú® One long paper about Temporal RAG Benchmark is accepted by Nature Scientific Data
- 2025.08: ‚ú® One long paper about Temporal RAG is accepted by CIKM 2025
- 2025.08: ‚ú® The long video generation work Animaker is accepted by ACM SIGGRAPH Asia 2025
- 2025.05: ‚ú® Unified multimodal LLMs Uni-MoE is accepted by IEEE TPAMI 2025
- 2025.01: ‚ú® GUI model UI-TARS are open-sourced
- 2024.11: ‚ú® Anim-Director is accepted by ACM SIGGRAPH Asia 2024
- 2024.05: ‚ú® Cognitive Visual-Knowledge Alignment work is accepted by ACL 2024 Main Conference
- 2024.04: ‚ú® VisionGraph is accepted by ICML 2024 Main Conference
- 2024.04: ‚ú® Multimodal LLMs LMEye is accepted by IEEE TMM 2024
- 2024.02: ‚ú® Multimodal E-commerce model is accepted by LREC-COLING 2024
- 2023.08: ‚ú® Multimodal Event Extraction work is accepted by ACM MM 2023
- 2023.05: ‚ú® Two multimodal reasoning papers are accepted by ACL 2023 Main Conference
- 2022.08: ‚ú® Chunk-aware reasoning work is accepted by ACM MM 2022
- 2022.05: ‚ú® Pivotal information recalling-based medical dialogue generation is accepted by SIGKDD
- 2022.03: ‚ú® Deep Spatial & Contextual Information network is accepted by IEEE TMM 2023


## üìï Selected Publications 

- **[Nature Scientific Data 2025]** A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation<br>
  <u><strong>Ziyang Chen</strong></u>, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Jichao Li, Shuaiqiang Wang, Baotian Hu, Dawei Yin
  <br>**[[pdf](https://arxiv.org/abs/2508.12282)]**  **[[code](https://github.com/czy1999/ChronoQA)]** 

- **[CIKM 2025]** Advancing Temporal Sensitive RAG through Progressive Multi-Step Reflection<br>
  <u><strong>Ziyang Chen</strong></u>, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Dawei Yin
  <br>**[[dataset](https://github.com/czy1999/ChronoReflect)]**  

- **[arXiv 2024]** An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&A Platforms<br>
  <u><strong>Ziyang Chen</strong></u>, Xiaobin Wang, Yong Jiang, Jinzhi Liao, Pengjun Xie, Fei Huang, Xiang Zhao
  <br>**[[pdf](https://arxiv.org/abs/2410.17694)]**  **[[code](https://github.com/czy1999/SynthRAG)]**  

- **[ACL 2024]** Temporal knowledge question answering via abstract reasoning induction<br>
  <u><strong>Ziyang Chen</strong></u>, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang
  <br>**[[pdf](https://aabs/2311.09149)]**  **[[code](https://github.com/czy1999/ARI-QA)]**  

  <!-- <img src="../images/ARI.png"  width="240" height="100"> -->

  <!-- <img src="../images/LLM.png"   width="200" height="100"> -->

- **[ACL 2023]** Multi-granularity temporal question answering over knowledge graphs<br>
   <u><strong>Ziyang Chen</strong></u>, Jinzhi Liao, Xiang Zhao
  <br>**[[pdf](https://aclanthology.org/2023.acl-long.637.pdf)]**  **[[code](https://github.com/czy1999/MultiTQ)]**  **[[data](https://huggingface.co/datasets/chenziyang/MultiTQ)]** 

  <!-- <img src="../images/MultiTQ.png"  width="230" height="100"> -->


- **[KBS 2022]** Temporal knowledge graph question answering via subgraph reasoning<br>
  <u><strong>Ziyang Chen</strong></u>, Xiang Zhao, Jinzhi Liao, Xinyi Li, Evangelos Kanoulas
  <br>**[[pdf](https://www.sciencedirect.com/science/article/pii/S0950705122005603)]**  **[[code](https://github.com/czy1999/SubGTR)]**
  
  <!-- <img src="../images/SubGTR.png"  width="250" height="140"> -->
  


## üíº Research Internship

- HKUST Research Assistant (2025.03 - 2025.08)
- ByteDance Doubao (Seed) Team (2024.10 - 2025.02)
- Tencent AILab (2024.04 - 2024.08)
- Tencent PCG (2021.10 - 2022.06)

## üíÅ Service

- Conference Reviewer: ACL ARR (2023-), ICLR (2023-), NeurIPS (2024-), ICML (2024-), AAAI (2024-), ACM SIGGRAPH (2025-), ACM MM (2023-), and IJCAI (2023-).
- Journal Reviewer: IEEE TPAMI, ACM Computing Survey, IEEE TIP, IEEE TMM, IEEE TNNLS, IEEE TCSVT, IEEE TAI, and Neural Networks.

## üèÖ Award

- Provincial Outstanding Graduates, 2019
- National Scholarship (2018, 2021, 2024)
- Baidu Scholarship (Global Top 40), 2024
- Huawei TopMinds, 2025
- JD TGT, 2025
- Tencent Qingyun, 2025
- Young Talent Support Project (Doctor), CAST, 2024
- Outstanding Doctoral Dissertation of HIT (ÂìàÂ∑•Â§ß‰ºòÂçö), 2025


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GGNQ9Q1DXW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GGNQ9Q1DXW');
</script>

<div id="mapContainer" style="width: 40%; margin: 0 auto;">
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=EHnMi1sl28eRT3YQedLX2Axxcw6-BakuDLa2DInHhFw&cl=ffffff&w=a"></script>
</div>


