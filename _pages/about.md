---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## üòé About Me (Êùé‰∫ëÈë´)

I just got my Ph.D. from Harbin Institute of Technology, Shenzhen and advised by Prof. [Baotian Hu](https://homepage.hit.edu.cn/hubaotian), Prof. [Yuxin Ding](https://homepage.hit.edu.cn/dingyuxin), and Prof. [Min Zhang](https://homepage.hit.edu.cn/MinZhang). I obtained a Master of Engineering degree from Harbin Institute of Technology, Shenzhen and a Bachelor of Science degree from Harbin Institute of Technology. Long-term cooperation with Dr. [Lin Ma](https://forestlinma.com/), Meituan, Beijing; Prof. [Wenhan Luo](https://whluo.github.io/), HKUST; Dr. [Longyue Wang](https://www.longyuewang.com/), Alibaba Group; Dr. [Yuxiang Wu](https://jimmycode.github.io/), University College London.

The long-term goal of my research is to help humans with more capable artificial intelligence. Dream of building an intelligent metaverse and interesting research directions including:
- Multimodal Collaborative Reasoning
- Video Understanding and Generation
- Multimodal Agent
- Embodied Intelligence

<p style="color: blue; margin: 0;">I am actively seeking cooperators who share my interest in developing large multimodal reasoning models to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.</p>


## üî• News
- 2025.10: ‚ú® One long paper about Temporal RAG Benchmark is accepted by Nature Scientific Data
- 2025.08: ‚ú® One long paper about Temporal RAG is accepted by CIKM 2025
- 2025.08: ‚ú® The long video generation work Animaker is accepted by [ACM SIGGRAPH Asia 2025](https://animaker-dev.github.io/)
- 2025.05: ‚ú® Unified multimodal LLMs Uni-MoE is accepted by [IEEE TPAMI 2025](https://ieeexplore.ieee.org/document/10887014)
- 2025.05: ‚ú® VideoVista-CulturalLingo is accepted by [ACL 2025 Main Conference](https://videovista-culturallingo.github.io/)
- 2025.01: ‚ú® Pioneering GUI model [UI-TARS](https://github.com/bytedance/UI-TARS) is open-sourced
- 2024.11: ‚ú® Anim-Director is accepted by [ACM SIGGRAPH Asia 2024](https://dl.acm.org/doi/abs/10.1145/3680528.3687688)
- 2024.05: ‚ú® Cognitive Visual-Knowledge Alignment work is accepted by [ACL 2024 Main Conference](https://aclanthology.org/2024.acl-long.411/)
- 2024.04: ‚ú® VisionGraph is accepted by [ICML 2024 Main Conference](https://proceedings.mlr.press/v235/li24ab.html)
- 2024.04: ‚ú® Multimodal LLMs LMEye is accepted by [IEEE TMM 2024](https://ieeexplore.ieee.org/document/10598361)
- 2024.02: ‚ú® Multimodal E-commerce model is accepted by [LREC-COLING 2024](https://aclanthology.org/2024.lrec-main.77/)
- 2023.08: ‚ú® Multimodal Event Extraction work is accepted by [ACM MM 2023](https://arxiv.org/abs/2306.08966)
- 2023.05: ‚ú® Two multimodal reasoning papers are accepted by [ACL 2023 Main Conference](https://aclanthology.org/people/yunxin-li/)
- 2022.08: ‚ú® Chunk-aware reasoning work is accepted by [ACM MM 2022](https://arxiv.org/abs/2207.11401)
- 2022.05: ‚ú® Pivotal information recalling-based medical dialogue generation is accepted by [SIGKDD](https://dl.acm.org/doi/10.1145/3534678.3542674)
- 2022.03: ‚ú® Deep Spatial & Contextual Information network is accepted by [IEEE TMM 2023](https://ieeexplore.ieee.org/document/9682541)


## üìï Selected Publications 

- **[Survey, ArXiv 2025]** Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models<br>
  <u><strong>Yunxin Li</strong></u>, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2505.04921)]** **[[web](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)]** **[[huggingface](https://huggingface.co/papers/2505.04921)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models.svg?style=social">

- **[IEEE TPAMI 2025]** Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts<br>
  <u><strong>Yunxin Li</strong></u>, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2405.11273)]** **[[web](https://uni-moe.github.io/)]** **[[code](https://github.com/HITsz-TMG/Uni-MoE)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Uni-MoE.svg?style=social">
  
- **[SIGGRAPH Asia 2024]** Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation<br>
  <u><strong>Yunxin Li</strong></u>, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2408.09787)]** **[[code](https://github.com/HITsz-TMG/Anim-Director)]**
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/HITsz-TMG/Anim-Director.svg?style=social">

- **[arXiv 2024]** VideoVista: A Versatile Benchmark for Video Understanding and Reasoning<br>
  <u><strong>Yunxin Li</strong></u>, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2406.11303)]** **[[web](https://videovista.github.io/)]** **[[code](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/VideoVista)]**

- **[ACL 2024]** Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment<br>
  <u><strong>Yunxin Li</strong></u>, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2402.13561)]** **[[code](https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper)]**

- **[ICML 2024]** VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2405.04950)]** **[[code](https://github.com/HITsz-TMG/VisionGraph)]**

- **[IEEE TMM 2024]** LMEye: An Interactive Perception Network for Large Language Models<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.03701)]** **[[code](https://github.com/YunxinLi/LingCloud)]**

- **[LREC-COLING 2024]** A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2402.13587)]** **[[code](https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning)]**

- **[arXiv 2023]** Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Wei Wang, Xiaochun Cao, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2311.15759)]**

- **[Technical Paper 2023]** A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering<br>
  <u><strong>Yunxin Li</strong></u>*, Longyue Wang*, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2311.07536)]** **[[code](https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper)]**

- **[ACM MM 2023]** Training Multimedia Event Extraction With Generated Images and Captions<br>
  Zilin Du, <u><strong>Yunxin Li</strong></u>, Xu Guo, Yidan Sun, Boyang Li<br>
  **[[pdf](https://arxiv.org/pdf/2306.08966.pdf)]** **[[code](https://github.com/ZILIN003/CAMEL)]**

- **[ACL 2023]** A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Yuxin Ding, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.02265)]** **[[code](https://github.com/YunxinLi/NDCR)]**

- **[ACL 2023]** A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues<br>
  <u><strong>Yunxin Li</strong></u>, Baotian Hu, Xinyu Chen, Yuxin Ding, Lin Ma, Min Zhang<br>
  **[[pdf](https://arxiv.org/abs/2305.04530)]** **[[code](https://github.com/YunxinLi/Multimodal-Context-Reasoning)]**

- **[ACM MM 2022]** Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations<br>
  Qian Yang*, <u><strong>Yunxin Li</strong></u>*, Baotian Hu, Lin Ma, Yuxing Ding, Min Zhang<br>
  **[[pdf](https://dl.acm.org/doi/abs/10.1145/3503161.3548284)]** **[[code](https://github.com/HITsz-TMG/ExplainableVisualEntailment)]**

- **[SIGKDD 2022]** Medical Dialogue Response Generation with Pivotal Information Recalling<br>
  Yu Zhao*, <u><strong>Yunxin Li</strong></u>*, Yuxiang Wu, Baotian Hu, Qingcai Chen, Xiaolong Wang, Yuxin Ding, Min Zhang<br>
  **[[pdf](https://dl.acm.org/doi/abs/10.1145/3534678.3542674)]** **[[code](https://github.com/HITsz-TMG/MedPIR)]**

- **[IEEE TMM 2022]** Fast and Robust Online Handwritten Chinese Character Recognition with Deep Spatial & Contextual Information Fusion Network<br>
  <u><strong>Yunxin Li</strong></u>, Qian Yang, Qingcai Chen, Baotian Hu, Xiaolong Wang, Yuxin Ding, Lin Ma<br>
  **[[pdf](https://ieeexplore.ieee.org/abstract/document/9682541)]**
  

## üíº Research Internship

- HKUST Research Assistant (2025.03 - 2025.08)
- ByteDance Doubao (Seed) Team (2024.10 - 2025.02)
- Tencent AILab (2024.04 - 2024.08)
- Tencent PCG (2021.10 - 2022.06)

## üíÅ Service

- Conference Reviewer: ACL ARR (2023-), ICLR (2023-), NeurIPS (2024-), ICML (2024-), AAAI (2024-), ACM SIGGRAPH (2025-), ACM MM (2023-), and IJCAI (2023-).
- Journal Reviewer: IEEE TPAMI, ACM Computing Survey, IEEE TIP, IEEE TMM, IEEE TNNLS, IEEE TCSVT, IEEE TAI, and Neural Networks.

## üèÖ Award

- Provincial Outstanding Graduates, 2019
- National Scholarship (2018, 2021, 2024)
- Baidu Scholarship (Global Top 40), 2024
- Huawei TopMinds (Âçé‰∏∫Â§©ÊâçÂ∞ëÂπ¥), 2025
- JD TGT (‰∫¨‰∏úÈ°∂Â∞ñÊäÄÊúØ‰∫∫ÊâçËÆ°Âàí), 2025
- Tencent Qingyun (ËÖæËÆØÈùí‰∫ë‰∫∫ÊâçËÆ°Âàí), 2025
- Young Talent Support Project-Doctor (È¶ñÂ±ä‰∏≠ÂõΩÁßëÂçèÈùíÊâòÂçöÂ£´ÁîüËÆ°Âàí), CAST, 2024
- Outstanding Doctoral Dissertation of HIT (ÂìàÂ∑•Â§ß‰ºòÂçö), 2025




